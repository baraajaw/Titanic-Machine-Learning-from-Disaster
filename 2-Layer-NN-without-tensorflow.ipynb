{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "- [numpy](https://www.numpy.org)\n",
    "- [pandas](https://pandas.pydata.org)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load the dataset\n",
    "Load the [dataset](https://www.kaggle.com/c/titanic/data), the dataset already split for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\", delimiter = ',')\n",
    "test_data = pd.read_csv(\"test.csv\", delimiter = ',')\n",
    "test_result = pd.read_csv(\"gender_submission.csv\", delimiter = ',')\n",
    "\n",
    "test_data[\"Survived\"] = test_result[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - prepare the dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(table):\n",
    "    X = table.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Survived\"], axis = 1)\n",
    "    Y = table[\"Survived\"]\n",
    "    \n",
    "    # fix \"Age\" \n",
    "    age_avg = round(X[\"Age\"].sum() / (len(X) - X[\"Age\"].isna().sum()), 1)\n",
    "    X[\"Age\"].fillna(age_avg, inplace = True)\n",
    "    #normalize Age\n",
    "    X[\"Age\"] = (X[\"Age\"] - X[\"Age\"].mean()) / X[\"Age\"].std()\n",
    "    \n",
    "    \n",
    "    # fix \"Fare\"\n",
    "    Fare_avg = round(X[\"Fare\"].sum() / (len(X) - X[\"Fare\"].isna().sum()), 1)\n",
    "    X[\"Fare\"].fillna(Fare_avg, inplace = True)\n",
    "    X[\"Fare\"] = (X[\"Fare\"] - X[\"Fare\"].mean()) / X[\"Fare\"].std()\n",
    "    \n",
    "    # fix Embarked \n",
    "    X[\"Embarked\"] = X[\"Embarked\"].map({'C' : 0.0, 'S' : 1.0, 'Q' : 2.0})\n",
    "    X['Embarked'].fillna(-1, inplace = True)\n",
    "    \n",
    "    #fix sex \n",
    "    X['Sex'] = X['Sex'].map({'female' : 0.0, 'male' : 1.0})\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X.values.T, Y.values.reshape(1, len(Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = prepare_data(train_data)\n",
    "X_test, Y_test = prepare_data(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Activation functions\n",
    "\n",
    "- [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "- [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    Z -- the input for the activation function\n",
    "    \n",
    "    Returns:\n",
    "    A -- the output of the activation function\n",
    "    cache -- dictionary containing Z\n",
    "    \"\"\"\n",
    "    A = 1.0 / (1.0 + np.exp(-Z))\n",
    "    cache = (Z)\n",
    "    \n",
    "    return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    Z -- the input for the activation function\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- gradients of the activations \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "\n",
    "    s = 1.0 / (1.0 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    Z -- the input for the activation function\n",
    "    \n",
    "    Returns:\n",
    "    A -- the output of the activation function\n",
    "    cache -- dictionary containing Z\n",
    "    \"\"\"\n",
    "    \n",
    "    A = Z * (Z > 0)\n",
    "    cache = (Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    Z -- the input for the activation function\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- gradients of the activations \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = dA * (Z > 0)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Initialization\n",
    "Initialize the weights matrices and biases vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- number of units in the input layer\n",
    "    n_h -- number of units in the hidden layer\n",
    "    n_y -- number of units in the hidden layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer\n",
    "    W -- weights matrix\n",
    "    b -- bias vector\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- dictionary containing A, W and b; stored to compute backward propagation step\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer\n",
    "    W -- weights matrix\n",
    "    b -- bias vector\n",
    "    activation -- the activation to be used in this layer\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the activation function\n",
    "    cache -- dictionary stored linear_cache and activation_cache to compute backward propagation step\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Yhat, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Yhat -- probabilities vector\n",
    "    Y -- labels vector\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(Yhat)) + np.multiply(1 - Y, np.log(1 - Yhat)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ -- gradients of activations\n",
    "    cache -- tuple of values (A_prev, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradients of activations\n",
    "    dW -- gradients of weights\n",
    "    db -- gradients of biases\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis= 1, keepdims= True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA -- gradients of activations\n",
    "    cache -- tuple of values (linear_cache, activation_cache)\n",
    "    activation -- activation function to use\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradients of activations\n",
    "    dW -- gradients of weights\n",
    "    db -- gradients of biases\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    \n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    parameters -- dictionary containing the parameters\n",
    "    grads -- dictionary contaning the gradients\n",
    "    learning_rate -- the learning rate\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing updated parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "  \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - The model\n",
    "Everything come together here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000):\n",
    "    \"\"\"\n",
    "    X -- training examples\n",
    "    Y -- training labels\n",
    "    layers_dims -- layers dimensions\n",
    "    learning_rate -- the learning rate\n",
    "    num_iterations -- number of iterations for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- the parameters for the final iteration\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        \n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        dA2 = -(np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        dA1, dW2, db2  = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA2, cache1, \"relu\")\n",
    "        \n",
    "        \n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i + 1, np.squeeze(cost)))\n",
    "        \n",
    "    return parameters\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 0.6828774561981352\n",
      "Cost after iteration 200: 0.6678030684920057\n",
      "Cost after iteration 300: 0.6460550024098107\n",
      "Cost after iteration 400: 0.6242610823755671\n",
      "Cost after iteration 500: 0.6049439815656301\n",
      "Cost after iteration 600: 0.5878589234151088\n",
      "Cost after iteration 700: 0.5725023463016533\n",
      "Cost after iteration 800: 0.5589844758384467\n",
      "Cost after iteration 900: 0.5471854603136336\n",
      "Cost after iteration 1000: 0.5372414351292499\n",
      "Cost after iteration 1100: 0.5289295758627119\n",
      "Cost after iteration 1200: 0.521969799903805\n",
      "Cost after iteration 1300: 0.5160715160656766\n",
      "Cost after iteration 1400: 0.5108690715467288\n",
      "Cost after iteration 1500: 0.5061433494174733\n",
      "Cost after iteration 1600: 0.501572733314198\n",
      "Cost after iteration 1700: 0.4974340595883999\n",
      "Cost after iteration 1800: 0.49364395301528635\n",
      "Cost after iteration 1900: 0.4901557332570256\n",
      "Cost after iteration 2000: 0.48684995225119143\n",
      "Cost after iteration 2100: 0.48368303307776483\n",
      "Cost after iteration 2200: 0.48027366516598097\n",
      "Cost after iteration 2300: 0.47712871185516914\n",
      "Cost after iteration 2400: 0.4741313924409554\n",
      "Cost after iteration 2500: 0.47135039761662617\n",
      "Cost after iteration 2600: 0.4687174455613511\n",
      "Cost after iteration 2700: 0.4663061383398845\n",
      "Cost after iteration 2800: 0.46415937679286323\n",
      "Cost after iteration 2900: 0.46216910899572294\n",
      "Cost after iteration 3000: 0.46034527930205094\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, layers_dims = (7, 8, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Prediction\n",
    "\n",
    "Use `X_test` and `Y_test` to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    X -- test examples\n",
    "    Y -- test labels\n",
    "    parameters: gradient descent parameters\n",
    "    \n",
    "    Returns:\n",
    "    percent -- the percentage of correction\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    A1, cache = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "    A2, cache = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    predictions = (Y == (A2 > 0.5))\n",
    "    percent = np.sum(predictions) / m * 100\n",
    "    \n",
    "    return percent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.17%\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:.2f}%\".format(predict(X_test, Y_test, parameters)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
